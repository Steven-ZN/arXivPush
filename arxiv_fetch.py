# arxiv_fetch.py
import arxiv, re
from dateutil.tz import gettz
from datetime import timedelta

def build_query(cfg):
    # æ„å»ºæ›´å®½æ¾çš„æœç´¢æŸ¥è¯¢

    # æ·»åŠ ä¸»è¦åˆ†ç±»ï¼ˆè¿™æ˜¯æœ€é‡è¦çš„ï¼‰
    cats = cfg.get("categories", [])
    if cats:
        # ä½¿ç”¨æ‰€æœ‰é…ç½®çš„åˆ†ç±»
        cat_query = " OR ".join([f'cat:{cat}' for cat in cats])
        base_query = f'({cat_query})'
    else:
        base_query = ""

    # åªæ·»åŠ 1-2ä¸ªæœ€å¸¸ç”¨çš„æœç´¢è¯ï¼Œé¿å…æŸ¥è¯¢è¿‡äºå¤æ‚
    terms = []
    for blk in cfg.get("queries", []):
        if "any" in blk:
            terms.extend(blk["any"])
        if "all" in blk:
            terms.extend(blk["all"])

    # é€‰æ‹©æœ€å¸¸ç”¨çš„1-2ä¸ªæœ¯è¯­
    common_terms = ["machine learning", "deep learning", "neural network"]
    selected_terms = []
    for term in common_terms:
        if term in [t.strip() for t in terms]:
            selected_terms.append(term)
            if len(selected_terms) >= 2:
                break

    # ç®€åŒ–æŸ¥è¯¢ï¼šåªä½¿ç”¨åˆ†ç±»ï¼Œä¸è¿‡æ»¤æœ¯è¯­ï¼Œä»¥è·å¾—æ›´å¤šè®ºæ–‡
    if base_query:
        return base_query
    else:
        # æœ€åå¤‡é€‰ï¼šåªæœç´¢æœ€æ–°è®ºæ–‡
        return ""


def iterative_time_aware_search(cfg, target=20, max_days=7):
    """
    æ—¶é—´æ„ŸçŸ¥çš„è¿­ä»£æœç´¢æ¶æ„
    ä¼˜å…ˆæŠ“å–æœ€æ–°è®ºæ–‡ï¼ŒåŠ¨æ€æ‰©å±•æ—¶é—´çª—å£ç›´åˆ°æ»¡è¶³æ¡ä»¶

    Args:
        cfg: é…ç½®å­—å…¸
        target: ç›®æ ‡è®ºæ–‡æ•°é‡
        max_days: æœ€å¤§æœç´¢å¤©æ•°

    Returns:
        list: æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åˆ—çš„è®ºæ–‡åˆ—è¡¨
    """
    from datetime import datetime, timedelta
    from dateutil.tz import gettz

    tz_local = gettz(cfg.get("timezone", "America/New_York"))
    current_date = datetime.now(tz_local).date()

    collected = []
    seen_ids = set()
    time_window = 1

    # è·å–æœç´¢é…ç½®
    cats = cfg.get("categories", ["cs.AI", "cs.LG", "cs.CL", "cs.CV"])
    excludes = [e.lower() for e in cfg.get("exclude", [])]

    print(f"ğŸ¯ å¯åŠ¨æ—¶é—´æ„ŸçŸ¥è¿­ä»£æœç´¢")
    print(f"ğŸ“… å½“å‰æ—¥æœŸ: {current_date}")
    print(f"ğŸ¯ ç›®æ ‡è®ºæ–‡: {target} ç¯‡")
    print(f"ğŸ” æœ€å¤§æœç´¢èŒƒå›´: {max_days} å¤©")
    print("=" * 60)

    while len(collected) < target and time_window <= max_days:
        # è®¡ç®—å½“å‰æœç´¢çª—å£
        end_date = current_date - timedelta(days=time_window-1)
        start_date = current_date - timedelta(days=time_window)

        print(f"ğŸ” [{time_window}/{max_days}] æœç´¢çª—å£: {start_date} ~ {end_date}")

        try:
            # æ„å»ºæ—¶é—´çª—å£æŸ¥è¯¢
            cat_query = " OR ".join([f'cat:{cat}' for cat in cats])

            # ä½¿ç”¨arxiv APIçš„æ—¶é—´è¿‡æ»¤
            search = arxiv.Search(
                query=cat_query,
                max_results=100,  # æ¯æ¬¡æœç´¢æ›´å¤šä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„é€‰æ‹©
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending,
            )

            # è·å–ç»“æœå¹¶è¿‡æ»¤
            batch_new_papers = []
            for r in arxiv.Client().results(search):
                # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº
                pub_local = r.published.astimezone(tz_local)
                pub_date = pub_local.date()

                # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰æœç´¢çª—å£å†…
                if not (start_date <= pub_date <= end_date):
                    continue

                # å»é‡ï¼ˆé¿å…ä¸åŒç‰ˆæœ¬çš„åŒä¸€è®ºæ–‡ï¼‰
                base_id = r.get_short_id().split('v')[0]
                if base_id in seen_ids:
                    continue
                seen_ids.add(base_id)

                # è¿‡æ»¤æ’é™¤è¯
                title = r.title.lower()
                abstract = (r.summary or '').lower()
                if any(e and (e in title or e in abstract) for e in excludes):
                    continue

                batch_new_papers.append(r)
                print(f"ğŸ“„ æ‰¾åˆ°è®ºæ–‡: {r.get_short_id()} - {r.title[:50]}...")

            # æ·»åŠ åˆ°æ”¶é›†åˆ—è¡¨
            collected.extend(batch_new_papers)

            print(f"âœ… çª—å£ {time_window}: æ–°å¢ {len(batch_new_papers)} ç¯‡, ç´¯è®¡ {len(collected)} ç¯‡")

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if len(collected) >= target:
                print(f"ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ {target} ç¯‡è®ºæ–‡!")
                break

        except Exception as e:
            print(f"âŒ çª—å£ {time_window} æœç´¢å¤±è´¥: {e}")
            # ç»§ç»­ä¸‹ä¸€ä¸ªçª—å£ï¼Œä¸ä¸­æ–­æ•´ä¸ªæœç´¢è¿‡ç¨‹

        # åŠ¨æ€æ‰©å±•æ—¶é—´çª—å£
        time_window += 1

        # é˜²æ­¢è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
        import time
        time.sleep(0.5)

    # æœ€ç»ˆæ’åºå’Œæˆªå–
    collected.sort(key=lambda x: x.published, reverse=True)
    final_results = collected[:target]

    print("\n" + "=" * 60)
    print("ğŸ“Š è¿­ä»£æœç´¢å®Œæˆ!")
    print(f"   - æœç´¢çª—å£æ•°: {time_window - 1}")
    print(f"   - æ€»è®ºæ–‡æ•°: {len(collected)}")
    print(f"   - æœ€ç»ˆé€‰å–: {len(final_results)} ç¯‡")

    if len(final_results) < target:
        print(f"   âš ï¸  æœªè¾¾åˆ°ç›®æ ‡ï¼Œåªæ‰¾åˆ° {len(final_results)} ç¯‡è®ºæ–‡")
    else:
        print(f"   âœ… æˆåŠŸè¾¾åˆ°ç›®æ ‡ {target} ç¯‡è®ºæ–‡")

    # æ˜¾ç¤ºæœ€æ–°è®ºæ–‡çš„å‘å¸ƒæ—¶é—´èŒƒå›´
    if final_results:
        latest = final_results[0].published.astimezone(tz_local)
        oldest = final_results[-1].published.astimezone(tz_local)
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {oldest.strftime('%Y-%m-%d')} ~ {latest.strftime('%Y-%m-%d')}")

    return final_results


def fetch_window(cfg, since_dt_local, now_local):
    """
    å…¼å®¹æ€§åŒ…è£…å™¨ï¼šä½¿ç”¨æ–°çš„æ—¶é—´æ„ŸçŸ¥è¿­ä»£æœç´¢
    """
    max_items = cfg.get("digest_max_items", 20)

    # ä½¿ç”¨æ–°çš„æ—¶é—´æ„ŸçŸ¥è¿­ä»£æœç´¢
    print(f"ğŸš€ ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥è¿­ä»£æœç´¢ (æ›¿ä»£ä¼ ç»Ÿæœç´¢)")

    try:
        results = iterative_time_aware_search(
            cfg=cfg,
            target=max_items,
            max_days=7
        )
        return results
    except Exception as e:
        print(f"âŒ æ—¶é—´æ„ŸçŸ¥æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæœç´¢: {e}")

        # å›é€€åˆ°ç®€åŒ–çš„ä¼ ç»Ÿæœç´¢
        return fallback_search(cfg, max_items)


def fallback_search(cfg, max_items):
    """
    ç®€åŒ–çš„å›é€€æœç´¢æ–¹æ¡ˆ
    """
    from datetime import datetime, timedelta
    from dateutil.tz import gettz

    tz_local = gettz(cfg.get("timezone", "America/New_York"))

    print(f"ğŸ”„ æ‰§è¡Œå›é€€æœç´¢æ–¹æ¡ˆ...")

    search = arxiv.Search(
        query="cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV",
        max_results=max_items,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending,
    )

    try:
        filtered_papers = []
        unique_ids = set()

        for r in arxiv.Client().results(search):
            paper_id = r.get_short_id()
            base_id = paper_id.split('v')[0]

            if base_id in unique_ids:
                continue
            unique_ids.add(base_id)

            # è¿‡æ»¤æ’é™¤è¯
            excludes = [e.lower() for e in cfg.get("exclude", [])]
            title = r.title.lower()
            abstract = (r.summary or '').lower()
            if any(e and (e in title or e in abstract) for e in excludes):
                continue

            filtered_papers.append(r)
            if len(filtered_papers) >= max_items:
                break

        print(f"âœ… å›é€€æœç´¢å®Œæˆ: {len(filtered_papers)} ç¯‡è®ºæ–‡")
        return filtered_papers

    except Exception as e:
        print(f"âŒ å›é€€æœç´¢ä¹Ÿå¤±è´¥: {e}")
        return []

def load_pushed_papers():
    """åŠ è½½å·²æ¨é€çš„è®ºæ–‡ID"""
    import os
    import json

    pushed_file = "pushed_papers.json"
    if os.path.exists(pushed_file):
        try:
            with open(pushed_file, "r") as f:
                data = json.load(f)
                return set(data.get("papers", []))
        except:
            return set()
    return set()

def save_pushed_papers(paper_ids):
    """ä¿å­˜å·²æ¨é€çš„è®ºæ–‡ID"""
    import os
    import json

    pushed_file = "pushed_papers.json"
    existing_ids = load_pushed_papers()
    all_ids = existing_ids.union(paper_ids)

    try:
        with open(pushed_file, "w") as f:
            json.dump({"papers": list(all_ids)}, f)
        print(f"ğŸ’¾ å·²ä¿å­˜ {len(paper_ids)} ä¸ªæ–°è®ºæ–‡IDåˆ°æ¨é€è®°å½•")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜æ¨é€è®°å½•å¤±è´¥: {e}")

def mark_papers_as_pushed(papers):
    """æ ‡è®°è®ºæ–‡ä¸ºå·²æ¨é€"""
    paper_ids = [p.get_short_id() for p in papers]
    save_pushed_papers(paper_ids)


def pack_papers(cfg, papers):
    # ç»“æ„åŒ–æˆ JSON å‹å¥½æ ¼å¼ï¼Œ paperså·²ç»åœ¨fetch_windowä¸­è¿‡å»é‡
    data = []
    max_abs = int(cfg.get("abstract_max_chars", 500))

    for p in papers:
        paper_id = p.get_short_id()
        abs_text = re.sub(r"\s+", " ", (p.summary or "")).strip()
        if len(abs_text) > max_abs:
            abs_text = abs_text[:max_abs] + "â€¦"

        # ç¡®ä¿æœ‰æ­£ç¡®çš„arXivé“¾æ¥
        arxiv_id = paper_id.split('v')[0]  # å»æ‰ç‰ˆæœ¬å·
        arxiv_link = f"https://arxiv.org/abs/{arxiv_id}"

        data.append({
            "id": paper_id,
            "title": p.title.strip().replace("\n", " "),
            "authors": [a.name for a in p.authors],
            "primary_category": p.primary_category,
            "published": p.published.isoformat(),
            "link": arxiv_link,  # ç»Ÿä¸€ä½¿ç”¨arxivé“¾æ¥
            "abstract": abs_text,
        })

    print(f"ğŸ“Š è®ºæ–‡æ•°é‡: {len(data)}")
    return data